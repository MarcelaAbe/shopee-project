# Recolecci√≥n de Tiendas Oficiales de Shopee por Categor√≠a

Este proyecto automatiza la recolecci√≥n de datos de **tiendas oficiales** de Shopee, segmentadas por categor√≠a, y su posterior procesamiento para normalizaci√≥n y carga en BigQuery.

---

## ü§ñ Resumen General

El proceso consta de dos etapas principales:

1. **Crawler en Node.js**  
   Se realiza la consulta a la API p√∫blica de Shopee por categor√≠a para obtener las tiendas oficiales. Los datos se recolectan en formato JSON y se exportan a CSV con timestamp.  

2. **Tratamiento y Normalizaci√≥n en Python**  
   El CSV generado es cargado en un DataFrame pandas para limpieza de nombres, normalizaci√≥n de cadenas, generaci√≥n de IDs √∫nicos (`UNIC_ID`) para marcas nuevas, verificaci√≥n y actualizaci√≥n con datos hist√≥ricos en BigQuery, y finalmente carga en una tabla BigQuery para an√°lisis posteriores.

---

## üì¶ Funcionalidades

### Crawler (Node.js)

- Solicita datos a la API p√∫blica de Shopee por categor√≠a  
- Usa headers simulando navegador real para evitar bloqueos  
- Recupera categor√≠as agrupadas y no agrupadas (tiendas nuevas)  
- Exporta CSV con timestamp  
- Preparado para integraci√≥n con BigQuery o bases externas

### Post-Procesamiento (Python)

- Normaliza nombres de usuario y marca (may√∫sculas, elimina s√≠mbolos y tildes)  
- Detecta y reporta caracteres especiales no deseados  
- Genera `UNIC_ID` alfanum√©rico √∫nico para tiendas nuevas  
- Consulta tabla hist√≥rica en BigQuery para reutilizar IDs existentes  
- Agrega columnas de auditor√≠a de fechas  
- Carga y actualiza tabla BigQuery con datos limpios y normalizados

---

## üß∞ Tecnolog√≠as Utilizadas

- **Node.js:** axios, dayjs, csv-writer  
- **Python:** pandas, re, unicodedata, melitk.bigquery (cliente BigQuery)  
- **BigQuery:** almacenamiento y consulta de datos finales  

---

## üìÇ Estructura y Resumen del C√≥digo

### 1. Crawler (crawler.js)

- Importa m√≥dulos axios, dayjs y csv-writer  
- Define headers HTTP para simular navegador  
- Lista de categor√≠as con IDs y nombres  
- Loop que hace petici√≥n API por cada categor√≠a, guardando tiendas oficiales  
- Exporta datos completos a CSV con fecha en el nombre  

### 2. Post-Procesamiento (Python)

- Carga CSV en DataFrame pandas  
- Normaliza columnas `USERNAME_SHOPEE` y `BRAND_NAME_SHOPEE`:  
  - May√∫sculas, reemplazo de signos (`.`, `_`, `-`) por espacio  
  - Elimina signos de puntuaci√≥n y acentos  
  - Sustituye `" & "` por `" E "`  
- Detecta caracteres especiales no permitidos y los reporta  
- Consulta tabla hist√≥rica en BigQuery para obtener IDs √∫nicos existentes  
- Para marcas nuevas genera IDs alfanum√©ricos √∫nicos (5 d√≠gitos)  
- Actualiza DataFrame con `UNIC_ID`  
- A√±ade columnas de auditor√≠a de inserci√≥n y actualizaci√≥n con timestamps  
- Convierte columna fecha scraping a formato datetime  
- Carga datos actualizados en tabla BigQuery para almacenamiento final  

---

## üìù Campos Exportados

El CSV y la tabla BigQuery contienen:

- index  
- total  
- username  
- brand_name  
- shopid  
- logo  
- logo_pc  
- shop_collection_id  
- ctime  
- brand_label  
- shop_type  
- redirect_url  
- entity_id  
- category_id  
- category_name  
- url_to  
- data_requisicao  
- UNIC_ID  
- AUD_INS_DTTM  
- AUD_UPD_DTTM  
- DATE_SCRAPING  

---

## ‚úÖ C√≥mo Usar

1. Clonar el repositorio  
2. Instalar dependencias Node.js:  
   ```bash
   npm install axios dayjs csv-writer
3. Ejecuta el script:
 `node crawler.js`  
5. El CSV ser√° guardado autom√°ticamente en la ra√≠z del proyecto con la fecha del d√≠a en el nombre.
6. Ejecutar script Python para post-procesamiento y carga a BigQuery:
  `node input_tabla_crawler.py`
7. Verificar resultados y datos en BigQuery

## üìå Notas

- La categor√≠a con ID "-1" es la p√°gina principal con tiendas no categorizadas
- Se recomienda eliminar duplicados en post-procesamiento
- El proceso genera IDs √∫nicos para las marcas nuevas, reutilizando las ya existentes


## üìé Posibles Extensiones

- Integraci√≥n directa con BigQuery para no generar csv 
- Automatizar la ejecuci√≥n peri√≥dica con cron
- Mejorar limpieza y validaci√≥n de datos
- Integraci√≥n con dashboard para monitoreo en tiempo real
- Manejo avanzado de duplicados y consolidaci√≥n de tiendas
